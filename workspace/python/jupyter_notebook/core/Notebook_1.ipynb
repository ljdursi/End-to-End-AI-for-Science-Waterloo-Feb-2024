{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ab2d9c",
   "metadata": {},
   "source": [
    "# Core: Training Physics-ML models using PhysicsNemo (Data-driven workflows)\n",
    "\n",
    "In this notebook, we will emulate a physical system governed by diffusion equation (Darcy flow) using a data-driven AI model.\n",
    "\n",
    "#### Contents of the Notebook\n",
    "- [Problem Statement](#Problem-Statement:-Developing-a-surrogate-model-for-the-Darcy-Flow-system)\n",
    "- [Fourier Neural Operator](#Fourier-Neural-Operator)\n",
    "- [Solving the Darcy Flow problem using FNO](#Solving-the-Darcy-Flow-Problem-using-FNO)\n",
    "    - [Step 1: Load data and initialize model](#Step-1:-Load-data-and-initialize-model)\n",
    "    - [Step 2: Setup optimizer and scheduler](#Step-2:-Setup-optimizer-and-scheduler)\n",
    "    - [Step 3: Setup utilities to validate the model](#Step-3:-Setup-utilities-to-validate-the-model)\n",
    "    - [Step 4: Setup training loop and train the model](#Step-4:-Setup-training-loop-and-train-the-model)\n",
    "\n",
    "#### Learning Objectives\n",
    "- How to use PhysicsNemo utilities to setup a data-driven training for physical systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11aa86",
   "metadata": {},
   "source": [
    "The data-driven approach involves using large datasets to train models and make predictions or decisions. These large datasets help neural networks learn the features present in unstructured data, and in effect \"learn the physics\" that governs the system. Training such models enables us to gain valuable insight about the system that is typically not possible or is computationally expensive to obtain using traditional techniques. An example of this can be predicting the transient response of the system given the initial condition - assuming the AI model is trained on enough dataset, we can assume that the model learns to capture the causality and other governing phenomena from the given dataset. The model is able to use the training to infer on an unseen initial condition and produce the outputs at much faster rate compared to the traditional techniques like solving the set of PDEs governing the system. A few other examples of this can be found in the domains of design optimization, inverse modeling, digital twin, among others.\n",
    "\n",
    "NVIDIA PhysicsNemo enables users to setup such problems with ease. PhysicsNemo is built on top of PyTorch and has several utilities and models specifically targeted towards Physics-ML. The utilities from PhysicsNemo are interoperable with PyTorch and provide the users flexibility to use PhysicsNemo to augment their existing deep learning or simulation workflows. In the following example, we will use a Fourier Neural Operator (FNO) model from NVIDIA PhysicsNemo and see how it can be used to develop a surrogate model that learns the mapping between the permeability and the pressure field of a Darcy flow system. The mapping learnt should be true for a distribution of permeability fields and not just a single solution. A typical approach for this would be to solve the governing equation for every new permeability field. However, in this surrogate modeling approach, we will train the model on enough pairs of input permeability and output pressure field such that it \"learns\" the correlation between the two (underlying physics) and uses that to accurately infer on any new unseen inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b69c5",
   "metadata": {},
   "source": [
    "## Problem Statement: Developing a surrogate model for the Darcy Flow system\n",
    "\n",
    "We will demonstrate the use of FNO on a 2D Darcy flow problem. The Darcy PDE is a second-order, elliptic PDE as shown below. One can also think of the Darcy PDE as a diffusion equation which can be used to parameterize a variety of systems including flow through porous media, elastic materials and heat conduction. \n",
    "\n",
    "\\begin{equation}\n",
    "-\\nabla \\cdot \\left(k(\\textbf{x})\\nabla u(\\textbf{x})\\right) = f(\\textbf{x}), \\quad \\textbf{x} \\in D,\n",
    "\\end{equation}\n",
    "\n",
    "in which $u(\\textbf{x})$ is the flow pressure, $k(\\textbf{x})$ is the permeability field and $f(\\cdot)$ is the\n",
    "forcing function. \n",
    "\n",
    "Here you will define the domain as a 2D unit square  $D=\\left\\{x,y \\in (0,1)\\right\\}$ with the boundary condition $u(\\textbf{x})=0, \\textbf{x}\\in\\partial D$. Recall that FNO requires a structured Euclidean input such that $D = \\textbf{x}_{i}$ where $i \\in \\mathbb{N}_{N\\times N}$. Thus both the permeability and flow fields are discretized into a 2D matrix $\\textbf{K}, \\textbf{U} \\in \\mathbb{R}^{N \\times N}$.\n",
    "This problem develops a surrogate model that learns the mapping between a permeability field and the pressure field,\n",
    "$\\textbf{K} \\rightarrow \\textbf{U}$, for a distribution of permeability fields $\\textbf{K} \\sim p(\\textbf{K})$.\n",
    "This is a key distinction compared to <em>solving the PDE</em>, you are <em>not</em> learning just a single solution but rather a <em>distribution</em>.\n",
    "\n",
    "<center><img src=\"images/darcy-problem-statement.png\" alt=\"Drawing\" style=\"width:600px\" /></center>\n",
    "\n",
    "\n",
    "We can attempt to solve this problem using a variety of neural network architectures. However, the choice of neural network can be improved by introducing inductive bias into the neural network design. PhysicsNemo contains a library of such models that have strong inductive bias which is useful for modeling problems in the physics-based domain. For this problem, we will use the Fourier Neural Operator which uses Fourier Transforms at the core which blends into a few key benefits like resolution invariance which we will soon see. Before we dive into the code, we will cover the theory behind FNOs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b125a",
   "metadata": {},
   "source": [
    "### Fourier Neural Operator\n",
    "\n",
    "Fourier neural operator (FNO) is a data-driven architecture which can be used to parameterize solutions for a distribution of PDE solutions. The key feature of FNO is the spectral convolutions: operations that place the integral kernel in Fourier space. \n",
    "\n",
    "<center><img src=\"images/fourier-layer.png\" alt=\"Drawing\" style=\"width:900px\" /></center>\n",
    "\n",
    "The spectral convolution (Fourier integral operator) is defined as follows:\n",
    "\\begin{equation}\n",
    "(\\mathcal{K}(\\mathbf{w})\\phi)(x) = \\mathcal{F}^{-1}(R_{\\mathbf{W}}\\cdot \\left(\\mathcal{F}\\right)\\phi)(x), \\quad \\forall x \\in D\n",
    "\\end{equation}\n",
    "where $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ are the forward and inverse Fourier transforms, respectively.\n",
    "$R_{\\mathbf{w}}$ is the transformation which contains the learnable parameters $\\mathbf{w}$. Note this operator is calculated\n",
    "over the entire <em>structured Euclidean</em> domain $D$ discretized with $n$ points.\n",
    "Fast Fourier Transform (FFT) is used to perform the Fourier transforms efficiently, and the resulting transformation $R_{\\mathbf{w}}$ is just a finite size matrix of learnable weights. Inside the spectral convolution, the Fourier coefficients are truncated to only the lower modes which in turn allows explicit control over the dimensionality of the spectral space and linear operator.\n",
    "The FNO model is the composition of a fully-connected \"lifting\" layer, $L$ spectral convolutions with point-wise linear skip connections and a decoding point-wise fully-connected neural network at the end.\n",
    "\\begin{equation}\n",
    "u_{net}(\\Phi;\\theta) = \\mathcal{Q}\\circ \\sigma(W_{L} + \\mathcal{K}_{L}) \\circ ... \\circ \\sigma(W_{1} + \\mathcal{K}_{1})\\circ \\mathcal{P}(\\Phi), \\quad \\Phi=\\left\\{\\phi(x); \\forall x \\in D\\right\\}\n",
    "\\end{equation}\n",
    "in which $\\sigma(W_{i} + \\mathcal{K}_{i})$ is the spectral convolution layer $i$ with the point-wise linear transform $W_{i}$ and activation function $\\sigma(\\cdot)$. $\\mathcal{P}$ is the point-wise lifting network that projects the input into a higher-dimensional latent space, $\\mathcal{P}: \\mathbb{R}^{d_in} \\rightarrow \\mathbb{R}^{k}$.\n",
    "Similarly $\\mathcal{Q}$ is the point-wise fully-connected decoding network, $\\mathcal{P}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d_out}$. Since all fully-connected components of FNO are point-wise operations, the model is invariant to the dimensionality of the input.\n",
    "\n",
    "<strong>Note:</strong> While FNO is technically invariant to the dimensionality of the discretized domain $D$, this domain <em>must</em> be a structured grid in Euclidean space. The inputs to FNO are analogous to images, but the model is invariant to the image resolution.\n",
    "\n",
    "<center><img src=\"images/fno-architecture.png\" alt=\"Drawing\" style=\"width:1000px\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3edd77",
   "metadata": {},
   "source": [
    "## Solving the Darcy Flow Problem using FNO\n",
    "\n",
    "With the problem definition and the theory explained, let's dive into training a FNO model for this problem. Let's start with a few imports. Here, we will import the model and some utilities for logging and checkpointing from PhysicsNemo while the rest will be from PyTorch and other libraries. In the subsequent sections we will see more utilities from PhysicsNemo and how they can simplify the problem definition of such scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RANK\"]=\"0\"\n",
    "os.environ[\"WORLD_SIZE\"]=\"1\"\n",
    "os.environ[\"MASTER_ADDR\"]=\"localhost\"\n",
    "os.environ[\"MASTER_PORT\"]=\"15678\"\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hydra.utils import to_absolute_path\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "from physicsnemo.models.fno import FNO\n",
    "from physicsnemo.launch.logging import LaunchLogger\n",
    "from physicsnemo.launch.utils.checkpoint import save_checkpoint\n",
    "from utils import HDF5MapStyleDataset\n",
    "\n",
    "from hydra import compose, initialize\n",
    "\n",
    "# load the hydra config\n",
    "initialize(version_base=\"1.3\", config_path=\"conf\")\n",
    "cfg = compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa18a39",
   "metadata": {},
   "source": [
    "### Step 1: Load data and initialize model\n",
    "\n",
    "For a data-driven problems, the first step is to load the data to be processed for training. Here the `HDF5MapStyleDataset` is a PyTorch `Dataset` and handles the data loading and the relevant transformations. The details of the `HDF5MapStyleDataset` are not relevant for this text, and hence we won't cover it in detail. Interested users can refer the [`utils.py`](./utils.py) file for more details. This dataset is then passed to a PyTorch `DataLoader` that samples the dataset and produces a batched output. \n",
    "\n",
    "The model is a FNO model which is imported from PhysicsNemo and is interoperable with other PyTorch utilities like dataloaders, optimizers etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LaunchLogger.initialize()\n",
    "\n",
    "dataset = HDF5MapStyleDataset(to_absolute_path(\"../../source_code/core/datasets/Darcy_241/train.hdf5\"), device=\"cuda\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model_branch = FNO(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    ").to(\"cuda\") # move the model to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63ac94",
   "metadata": {},
   "source": [
    "### Step 2: Setup optimizer and scheduler\n",
    "\n",
    "Next, we initialize an Adam optimizer to train the weights of the model. The optimizer updates the weights of the neural network based on the loss gradient information. We will use the default Adam optimizer from PyTorch for this purpose. Typically, it is a good practice to reduce the learning rate as the model is trained longer, this helps to minimize the oscillations and improves convergence. For this example, an exponentially decaying learning rate is used to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    chain(model_branch.parameters()),\n",
    "    betas=(0.9, 0.999),\n",
    "    lr=cfg.start_lr,\n",
    "    weight_decay=0.0,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=cfg.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571484c",
   "metadata": {},
   "source": [
    "### Step 3: Setup utilities to validate the model\n",
    "\n",
    "We are almost ready to train our model, but before we do that, we would like to evaluate the model on a dataset that is not part of the training to gage it's out-of-sample performance. This can be done by a validation/test dataset. We will also define a `validation_step` that computes some metrics like validation error and plots the results on these samples as a part of that process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = HDF5MapStyleDataset(to_absolute_path(\"../../source_code/core/datasets/Darcy_241/validation.hdf5\"), device=\"cuda\")\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "os.makedirs(\"./results/\", exist_ok=True)\n",
    "\n",
    "def validation_step(model, dataloader, epoch):\n",
    "    \"\"\"Validation Step\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        for data in dataloader:\n",
    "            invar, outvar, _, _ = data\n",
    "            out = model(invar[:, 0].unsqueeze(dim=1))\n",
    "\n",
    "            loss_epoch += F.mse_loss(outvar, out)\n",
    "\n",
    "        # convert data to numpy\n",
    "        outvar = outvar.detach().cpu().numpy()\n",
    "        predvar = out.detach().cpu().numpy()\n",
    "\n",
    "        # plotting\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(25, 5))\n",
    "\n",
    "        d_min = np.min(outvar[0, 0, ...])\n",
    "        d_max = np.max(outvar[0, 0, ...])\n",
    "\n",
    "        im = ax[0].imshow(outvar[0, 0, ...], vmin=d_min, vmax=d_max)\n",
    "        plt.colorbar(im, ax=ax[0])\n",
    "        im = ax[1].imshow(predvar[0, 0, ...], vmin=d_min, vmax=d_max)\n",
    "        plt.colorbar(im, ax=ax[1])\n",
    "        im = ax[2].imshow(np.abs(predvar[0, 0, ...] - outvar[0, 0, ...]))\n",
    "        plt.colorbar(im, ax=ax[2])\n",
    "\n",
    "        ax[0].set_title(\"True\")\n",
    "        ax[1].set_title(\"Pred\")\n",
    "        ax[2].set_title(\"Difference\")\n",
    "        \n",
    "        fig.savefig(f\"./results/results_{epoch}.png\")\n",
    "        plt.close()\n",
    "        return loss_epoch / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b431378",
   "metadata": {},
   "source": [
    "### Step 4: Setup training loop and train the model\n",
    "\n",
    "Great, now we are ready to start training our FNO model! This process typically involves computing the loss (difference between the model prediction and truth) across the entire training dataset and repeating this process for a few epochs until a desired accuracy is reached. Here, we define the training loop as follows: we use one loop to iterate through all the training data (remember we are using a batched dataloader) and another loop to repeat the process for a few training epochs. \n",
    "\n",
    "The inner loop enumerates data from the DataLoader, and on each pass of the loop does the following:\n",
    "\n",
    "1. Zeros the optimizer’s gradients\n",
    "2. Gets a batch of training data from the DataLoader\n",
    "3. Performs an inference - that is, gets predictions from the model for an input batch\n",
    "4. Calculates the loss for that set of predictions vs. the labels on the dataset\n",
    "5. Calculates the backward gradients over the learning weights\n",
    "6. Tells the optimizer to perform one learning step (adjust the model’s learning weights based on the observed gradients for this batch, according to the optimization algorithm chosen)\n",
    "7. Adjusts the learning rate based on the chosen scheduler scheme\n",
    "\n",
    "This is a typical PyTorch training workflow and for more information, you may refer [here](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17703a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(cfg.max_epochs):\n",
    "    # wrap epoch in launch logger for console logs\n",
    "    with LaunchLogger(\n",
    "        \"train\",\n",
    "        epoch=epoch,\n",
    "        num_mini_batch=len(dataloader),\n",
    "        epoch_alert_freq=10,\n",
    "    ) as log:\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            truevar = data[1]\n",
    "            \n",
    "            # compute forward pass\n",
    "            outvar = model_branch(data[0][:, 0].unsqueeze(dim=1))\n",
    "            \n",
    "            # compute data loss\n",
    "            loss_data = F.mse_loss(outvar, truevar)\n",
    "            \n",
    "            # Compute total loss\n",
    "            loss = loss_data\n",
    "            \n",
    "            # Backward pass and optimizer and learning rate update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        log.log_epoch({\"Learning Rate\": optimizer.param_groups[0][\"lr\"]})\n",
    "    \n",
    "    # test model on validation dataset\n",
    "    with LaunchLogger(\"valid\", epoch=epoch) as log:\n",
    "        error = validation_step(model_branch, validation_dataloader, epoch)\n",
    "        log.log_epoch({\"Validation error\": error})\n",
    "\n",
    "    # save checkpoint\n",
    "    save_checkpoint(\n",
    "        \"./checkpoints\",\n",
    "        models=[model_branch],\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epoch=epoch,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380134c5",
   "metadata": {},
   "source": [
    "You can now visualize the results of the training by looking at the outputs from the `results` directory. That completes our introductory example on training a data-driven model. As a next step, we will take these learnings and train a data-driven global weather prediction model. Please continue to [the next notebook](../weather/Notebook_2.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8648b2",
   "metadata": {},
   "source": [
    "# Important: Free up GPU Memory!\n",
    "\n",
    "Run the below cell to free up GPU memory after training the model before moving to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982351d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35d08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "--- \n",
    "\n",
    "Don't forget to check out additional [Open Hackathons Resources](https://www.openhackathons.org/s/technical-resources) and join our [OpenACC and Hackathons Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "# Licensing\n",
    "\n",
    "Copyright © 2023 OpenACC-Standard.org.  This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
